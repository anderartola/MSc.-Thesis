\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Deep Learning the Lyman-alpha forest}{25}{chapter.80}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap: deep learning}{{3}{25}{Deep Learning the Lyman-alpha forest}{chapter.80}{}}
\newlabel{chap: deep learning@cref}{{[chapter][3][]3}{[1][25][]25}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction and motivation for the use of Deep Learning}{25}{section.81}\protected@file@percent }
\newlabel{sec:motiv_ml}{{3.1}{25}{Introduction and motivation for the use of Deep Learning}{section.81}{}}
\newlabel{sec:motiv_ml@cref}{{[section][1][3]3.1}{[1][25][]25}}
\newlabel{fig:MLP}{{3.1a}{26}{Graph for a simple MLP with a hidden layer (in orange) and an output neuron (in cyan).\relax }{figure.caption.84}{}}
\newlabel{fig:MLP@cref}{{[subfigure][1][3,1]3.1a}{[1][26][]26}}
\newlabel{sub@fig:MLP}{{a}{26}{Graph for a simple MLP with a hidden layer (in orange) and an output neuron (in cyan).\relax }{figure.caption.84}{}}
\newlabel{sub@fig:MLP@cref}{{[subfigure][1][3,1]3.1a}{[1][26][]26}}
\newlabel{fig:MLP_approx}{{3.1b}{26}{Unit impulse (in black) and the output of the MLP shown in cyan approximating the impulse. \relax }{figure.caption.84}{}}
\newlabel{fig:MLP_approx@cref}{{[subfigure][2][3,1]3.1b}{[1][26][]26}}
\newlabel{sub@fig:MLP_approx}{{b}{26}{Unit impulse (in black) and the output of the MLP shown in cyan approximating the impulse. \relax }{figure.caption.84}{}}
\newlabel{sub@fig:MLP_approx@cref}{{[subfigure][2][3,1]3.1b}{[1][26][]26}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces A simple multilayer perceptron (MLP) with a single hidden layer and a $\tanh {}$ activation function is able to approximate a unit pulse function. From left to right and top to bottom, the three biases are $\{0, 20, 0\}$ and the four weights are $\{20, -20, 1/2, 1/2\}$.\relax }}{26}{figure.caption.84}\protected@file@percent }
\newlabel{fig:ML MLP approx}{{3.1}{26}{A simple multilayer perceptron (MLP) with a single hidden layer and a $\tanh {}$ activation function is able to approximate a unit pulse function. From left to right and top to bottom, the three biases are $\{0, 20, 0\}$ and the four weights are $\{20, -20, 1/2, 1/2\}$.\relax }{figure.caption.84}{}}
\newlabel{fig:ML MLP approx@cref}{{[figure][1][3]3.1}{[1][26][]26}}
\newlabel{th:aprox}{{1}{26}{Universal approximation theorem}{theorem.83}{}}
\newlabel{th:aprox@cref}{{[theorem][1][]1}{[1][26][]26}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Evolution of the largest supercomputers in the \href  {https://top500.org}{\textsc  {TOP500}} list in recent years (x-axis). The y-axis shows the peak performance in GFLOPS for the first-ranked computer (red), the last-ranked computer (yellow), and the cumulative power for the top-500 computers (blue). Source: \href  {https://en.wikipedia.org/wiki/History_of_supercomputing}{Wikipedia}.\relax }}{27}{figure.caption.85}\protected@file@percent }
\newlabel{fig:ML_HPC}{{3.2}{27}{Evolution of the largest supercomputers in the \href {https://top500.org}{\textsc {TOP500}} list in recent years (x-axis). The y-axis shows the peak performance in GFLOPS for the first-ranked computer (red), the last-ranked computer (yellow), and the cumulative power for the top-500 computers (blue). Source: \href {https://en.wikipedia.org/wiki/History_of_supercomputing}{Wikipedia}.\relax }{figure.caption.85}{}}
\newlabel{fig:ML_HPC@cref}{{[figure][2][3]3.2}{[1][27][]27}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Histogram showing the number of SNe discovered each year as given by the Asiago Supernova Catalogue.\relax }}{29}{figure.caption.89}\protected@file@percent }
\newlabel{fig:Supernovas_per_year}{{3.3}{29}{Histogram showing the number of SNe discovered each year as given by the Asiago Supernova Catalogue.\relax }{figure.caption.89}{}}
\newlabel{fig:Supernovas_per_year@cref}{{[figure][3][3]3.3}{[1][28][]29}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Figure extracted from \blx@tocontentsinit {0}\cite {wdm_from_field}. The left panel shows a sample image density field used in the training data by the authors, with varied image resolution. The right panel shows a sample a predicted WDM masses versus the true WDM mass of the simulation for their fiducial neural network, which can accurately recover the WDM model within a 1 KeV accuracy up to 10 KeV.\relax }}{31}{figure.caption.90}\protected@file@percent }
\newlabel{fig:ML paper wdm field}{{3.4}{31}{Figure extracted from \cite {wdm_from_field}. The left panel shows a sample image density field used in the training data by the authors, with varied image resolution. The right panel shows a sample a predicted WDM masses versus the true WDM mass of the simulation for their fiducial neural network, which can accurately recover the WDM model within a 1 KeV accuracy up to 10 KeV.\relax }{figure.caption.90}{}}
\newlabel{fig:ML paper wdm field@cref}{{[figure][4][3]3.4}{[1][30][]31}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Fundamentals of (Bayesian) Neural Networks}{31}{section.92}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Figure extracted from \blx@tocontentsinit {0}\cite {lynna} showing the performance of the neural network recovering thermal parameters on a set of unseen skewers. The true parameter values, shown as dashed lines, are recovered by the average of the point predictions, shown as the dark green cross.\relax }}{32}{figure.caption.91}\protected@file@percent }
\newlabel{fig:ML LYNNA}{{3.5}{32}{Figure extracted from \cite {lynna} showing the performance of the neural network recovering thermal parameters on a set of unseen skewers. The true parameter values, shown as dashed lines, are recovered by the average of the point predictions, shown as the dark green cross.\relax }{figure.caption.91}{}}
\newlabel{fig:ML LYNNA@cref}{{[figure][5][3]3.5}{[1][31][]32}}
\newlabel{eq:var_noise_model}{{3.3}{33}{Fundamentals of (Bayesian) Neural Networks}{equation.94}{}}
\newlabel{eq:var_noise_model@cref}{{[equation][3][3]3.3}{[1][33][]33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Dataset generation, data augmentation and overfitting}{34}{subsection.100}\protected@file@percent }
\newlabel{sec: dataset}{{3.2.1}{34}{Dataset generation, data augmentation and overfitting}{subsection.100}{}}
\newlabel{sec: dataset@cref}{{[subsection][1][3,2]3.2.1}{[1][34][]34}}
\newlabel{eq_ch3:global_loss}{{3.5}{35}{Dataset generation, data augmentation and overfitting}{equation.102}{}}
\newlabel{eq_ch3:global_loss@cref}{{[equation][5][3]3.5}{[1][34][]35}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces The left panel shows the data points (with added noise) generated from a function $f$ in black. Three generative polynomial models are fitted to the data: linear regression in orange, and two smoothing splines in red and blue. The right panel shows the fitting error as a function of the polynomial degree, i.e., the flexibility of the model. The gray curve shows the error on the training dataset, which is monotonically decreasing. The red curve shows the error on the validation dataset, which initially decreases but then grows as the model overfits the training data. Figure extracted from \blx@tocontentsinit {0}\cite {James2021}. \relax }}{36}{figure.caption.104}\protected@file@percent }
\newlabel{fig:ML overfit poly}{{3.6}{36}{The left panel shows the data points (with added noise) generated from a function $f$ in black. Three generative polynomial models are fitted to the data: linear regression in orange, and two smoothing splines in red and blue. The right panel shows the fitting error as a function of the polynomial degree, i.e., the flexibility of the model. The gray curve shows the error on the training dataset, which is monotonically decreasing. The red curve shows the error on the validation dataset, which initially decreases but then grows as the model overfits the training data. Figure extracted from \cite {James2021}. \relax }{figure.caption.104}{}}
\newlabel{fig:ML overfit poly@cref}{{[figure][6][3]3.6}{[1][35][]36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Deep learning architecture}{37}{subsection.106}\protected@file@percent }
\newlabel{sec:deep learning archi}{{3.2.2}{37}{Deep learning architecture}{subsection.106}{}}
\newlabel{sec:deep learning archi@cref}{{[subsection][2][3,2]3.2.2}{[1][36][]37}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Max pooling operation with kernel size $(2,2)$\relax }}{39}{figure.caption.112}\protected@file@percent }
\newlabel{fig:ML max pool}{{3.7}{39}{Max pooling operation with kernel size $(2,2)$\relax }{figure.caption.112}{}}
\newlabel{fig:ML max pool@cref}{{[figure][7][3]3.7}{[1][39][]39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Prediction uncertainty and Bayesian models}{39}{subsection.113}\protected@file@percent }
\newlabel{eq:Bayes theorem}{{3.12}{39}{Prediction uncertainty and Bayesian models}{equation.114}{}}
\newlabel{eq:Bayes theorem@cref}{{[equation][12][3]3.12}{[1][39][]39}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Illustration for a non-stochastic neural network a), a network with stochastic activations b), and a network with stochastic weights in c). Source: \blx@tocontentsinit {0}\cite {BNN_review}.\relax }}{40}{figure.caption.115}\protected@file@percent }
\newlabel{fig:ML BNN ilus}{{3.8}{40}{Illustration for a non-stochastic neural network a), a network with stochastic activations b), and a network with stochastic weights in c). Source: \cite {BNN_review}.\relax }{figure.caption.115}{}}
\newlabel{fig:ML BNN ilus@cref}{{[figure][8][3]3.8}{[1][40][]40}}
\newlabel{eq:posterior precitive}{{3.14}{40}{Prediction uncertainty and Bayesian models}{equation.117}{}}
\newlabel{eq:posterior precitive@cref}{{[equation][14][3]3.14}{[1][40][]40}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Hyperparameter selection}{41}{subsection.119}\protected@file@percent }
\newlabel{sec:optuna}{{3.2.4}{41}{Hyperparameter selection}{subsection.119}{}}
\newlabel{sec:optuna@cref}{{[subsection][4][3,2]3.2.4}{[1][41][]41}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Example DE estimation for the distribution $l(x)$ and $g(x)$ used in the TPE algorithm.\relax }}{42}{figure.caption.125}\protected@file@percent }
\newlabel{fig:ML TPE}{{3.9}{42}{Example DE estimation for the distribution $l(x)$ and $g(x)$ used in the TPE algorithm.\relax }{figure.caption.125}{}}
\newlabel{fig:ML TPE@cref}{{[figure][9][3]3.9}{[1][42][]42}}
\newlabel{eq:expected imp}{{3.15}{42}{Hyperparameter selection}{equation.120}{}}
\newlabel{eq:expected imp@cref}{{[equation][15][3]3.15}{[1][42][]42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.5}Loss function and training}{43}{subsection.128}\protected@file@percent }
\newlabel{eq:euclidean norm}{{3.19}{43}{Loss function and training}{equation.130}{}}
\newlabel{eq:euclidean norm@cref}{{[equation][19][3]3.19}{[1][43][]43}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Classical deep learning training loop\relax }}{44}{algorithm.131}\protected@file@percent }
\newlabel{alg:training loop}{{1}{44}{Classical deep learning training loop\relax }{algorithm.131}{}}
\newlabel{alg:training loop@cref}{{[algorithm][1][]1}{[1][44][]44}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Adam optimizer\relax }}{45}{algorithm.143}\protected@file@percent }
\newlabel{alg:adam}{{2}{45}{Adam optimizer\relax }{algorithm.143}{}}
\newlabel{alg:adam@cref}{{[algorithm][2][]2}{[1][44][]45}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Distributed mirrored learning strategy with two workers (GPUs) and a CPU agregating the gradient and updating the model parameters.\relax }}{46}{figure.caption.156}\protected@file@percent }
\newlabel{fig:ML mirror training}{{3.10}{46}{Distributed mirrored learning strategy with two workers (GPUs) and a CPU agregating the gradient and updating the model parameters.\relax }{figure.caption.156}{}}
\newlabel{fig:ML mirror training@cref}{{[figure][10][3]3.10}{[1][46][]46}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Workflow implementation: Recovering IGM conditions from the Lyman-alpha forest}{46}{section.157}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Fiducial architecture for our Bayesian neural network trained at $z=4.4$ on the Sherwodd simulation suite. The fiducial parameters can be found in \cref  {table: fiducial architecture}.\relax }}{48}{figure.caption.159}\protected@file@percent }
\newlabel{fig:ML nn architecture}{{3.11}{48}{Fiducial architecture for our Bayesian neural network trained at $z=4.4$ on the Sherwodd simulation suite. The fiducial parameters can be found in \cref {table: fiducial architecture}.\relax }{figure.caption.159}{}}
\newlabel{fig:ML nn architecture@cref}{{[figure][11][3]3.11}{[1][47][]48}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Hyper-parameter grid search for the fiducial model at $z=4.4$. ``$\log _s$'' indicates the parameter is sampled in the log domain. ``Int'' and ``float'' mean they are sampled as integers or floats, respectively.\relax }}{49}{table.caption.161}\protected@file@percent }
\newlabel{table: fiducial architecture}{{3.1}{49}{Hyper-parameter grid search for the fiducial model at $z=4.4$. ``$\log _s$'' indicates the parameter is sampled in the log domain. ``Int'' and ``float'' mean they are sampled as integers or floats, respectively.\relax }{table.caption.161}{}}
\newlabel{table: fiducial architecture@cref}{{[table][1][3]3.1}{[1][48][]49}}
\newlabel{eq:our loss}{{3.22}{49}{Workflow implementation: Recovering IGM conditions from the Lyman-alpha forest}{equation.162}{}}
\newlabel{eq:our loss@cref}{{[equation][22][3]3.22}{[1][49][]49}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Learning curve for our fiducial architecture at $z=4.4$ on the \texttt  {SHERWOOD} dataset. The figure shows the NLL and the MAE on the validation split as a function of the epoch.\relax }}{50}{figure.caption.163}\protected@file@percent }
\newlabel{fig:ML learning curve}{{3.12}{50}{Learning curve for our fiducial architecture at $z=4.4$ on the \texttt {SHERWOOD} dataset. The figure shows the NLL and the MAE on the validation split as a function of the epoch.\relax }{figure.caption.163}{}}
\newlabel{fig:ML learning curve@cref}{{[figure][12][3]3.12}{[1][50][]50}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces An example 20h$^{-1}$cMpc Lyman-$\alpha $ validation skewer for the CDM and WDM3 Sherwood models. The top panel shows the input flux to the network. The bottom panel shows the true $\Delta _\tau $ density fields, the (mean) recovered densities and the $1\sigma $ envelope predicted by the Bayesian network.\relax }}{51}{figure.caption.164}\protected@file@percent }
\newlabel{fig: example_recovered_skewer}{{3.13}{51}{An example 20h$^{-1}$cMpc Lyman-$\alpha $ validation skewer for the CDM and WDM3 Sherwood models. The top panel shows the input flux to the network. The bottom panel shows the true $\Delta _\tau $ density fields, the (mean) recovered densities and the $1\sigma $ envelope predicted by the Bayesian network.\relax }{figure.caption.164}{}}
\newlabel{fig: example_recovered_skewer@cref}{{[figure][13][3]3.13}{[1][51][]51}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Recovered field statistics and uncertainties}{51}{section.165}\protected@file@percent }
\newlabel{sec:recovered statistics}{{3.4}{51}{Recovered field statistics and uncertainties}{section.165}{}}
\newlabel{sec:recovered statistics@cref}{{[section][4][3]3.4}{[1][51][]51}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Noisy regions and masking}{52}{subsection.166}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces Model $\Delta _\tau $ PDFs for the \texttt  {SHERWOOD} runs computed using the recovered density by out neural network. We explicitly highlight the effect of masking the satured regions that have a flux $\leq 3\frac  {1}{\mathrm  {SNR}}$. \relax }}{53}{figure.caption.167}\protected@file@percent }
\newlabel{fig: PDF masked unmasked}{{3.14}{53}{Model $\Delta _\tau $ PDFs for the \texttt {SHERWOOD} runs computed using the recovered density by out neural network. We explicitly highlight the effect of masking the satured regions that have a flux $\leq 3\frac {1}{\mathrm {SNR}}$. \relax }{figure.caption.167}{}}
\newlabel{fig: PDF masked unmasked@cref}{{[figure][14][3]3.14}{[1][53][]53}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Uncertainty in the recovered statistics}{53}{subsection.168}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Sampling from reconstructed density fields}{53}{subsubsection*.170}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces An example \texttt  {SHERWOOD} CDM skewer, together with the reconstructed $\Delta _\tau $ in blue. The red field shows a resampling without including covariance between the pixels, which genreates random noise.\relax }}{54}{figure.caption.171}\protected@file@percent }
\newlabel{fig: resampling indep}{{3.15}{54}{An example \texttt {SHERWOOD} CDM skewer, together with the reconstructed $\Delta _\tau $ in blue. The red field shows a resampling without including covariance between the pixels, which genreates random noise.\relax }{figure.caption.171}{}}
\newlabel{fig: resampling indep@cref}{{[figure][15][3]3.15}{[1][54][]54}}
\newlabel{eq: residuals}{{3.23}{54}{Sampling from reconstructed density fields}{equation.172}{}}
\newlabel{eq: residuals@cref}{{[equation][23][3]3.23}{[1][54][]54}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces The $\Delta _\tau $ correlation matrix $\Sigma $ for the prediction \texttt  {SHERWOOD} CDM residuals as defined in Equation \ref {eq: residuals} at $z=4.4$ for the fiducial trained neural network. Each reconstructed density field has been augmented by applying 1000 translations to generate a smooth matrix.\relax }}{55}{figure.caption.173}\protected@file@percent }
\newlabel{fig: corr mat cdm}{{3.16}{55}{The $\Delta _\tau $ correlation matrix $\Sigma $ for the prediction \texttt {SHERWOOD} CDM residuals as defined in Equation \ref {eq: residuals} at $z=4.4$ for the fiducial trained neural network. Each reconstructed density field has been augmented by applying 1000 translations to generate a smooth matrix.\relax }{figure.caption.173}{}}
\newlabel{fig: corr mat cdm@cref}{{[figure][16][3]3.16}{[1][54][]55}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.17}{\ignorespaces A set of 100 draws including the correlation between neraby pixels for a \texttt  {SHERWOOD} CDM skewer. As opposed to Figure \ref {fig: resampling indep}, the samples are continous and no longer show a noisy behavior.\relax }}{55}{figure.caption.174}\protected@file@percent }
\newlabel{fig: resampling corr}{{3.17}{55}{A set of 100 draws including the correlation between neraby pixels for a \texttt {SHERWOOD} CDM skewer. As opposed to Figure \ref {fig: resampling indep}, the samples are continous and no longer show a noisy behavior.\relax }{figure.caption.174}{}}
\newlabel{fig: resampling corr@cref}{{[figure][17][3]3.17}{[1][54][]55}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Intrinsic scatter in the recovered PDF}{56}{subsubsection*.176}\protected@file@percent }
\newlabel{eq:PDF of PDF}{{3.27}{56}{Intrinsic scatter in the recovered PDF}{equation.180}{}}
\newlabel{eq:PDF of PDF@cref}{{[equation][27][3]3.27}{[1][56][]56}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.18}{\ignorespaces \relax }}{57}{figure.caption.181}\protected@file@percent }
\newlabel{fig: boot vs exact}{{3.18}{57}{\relax }{figure.caption.181}{}}
\newlabel{fig: boot vs exact@cref}{{[figure][18][3]3.18}{[1][56][]57}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.19}{\ignorespaces $\Delta _\tau $ PDF computed over the fu