\chapter{Conclusions}
In this work, we have explored the potential of novel machine learning techniques to constrain warm dark matter in the cosmological context. We began in Section \ref{chapter:intro} with the basics of the current cosmological paradigm. We highlighted the importance of the intergalactic medium in the formation and evolution of structures. Then, we elaborated on how the Lyman-$\alpha$ forest observed on the spectra of quasars can serve as an efficient probe of the state of the IGM, allowing for the extraction of valuable information. We noted the relevance of dark matter in the $\Lambda$CDM model and discussed the key concepts related to it, such as the free-streaming length. We then understood the motivation behind considering alternative models to CDM, such as warm dark matter. Since WDM affects the matter distribution in the IGM, the Lyman-$\alpha$ forest can provide information on its nature. To understand the impact of WDM on the Lyman-$\alpha$ forest, we turned our attention in Section \ref{chap:sherwood} to cosmological simulations. We introduced the key points on how such hydrodynamical simulations are performed and introduced the \texttt{SHERWOOD} simulation suite, which is at the core of this work. Afterwards, we used the \texttt{SHERWOOD} raw data to compute simulated Lyman-$\alpha$ skewers and explored the impact of peculiar velocities and warm dark matter on its structure. For the latter, we introduced two statistics, the power spectrum and the probability distribution function, to understand the flux and density field from a statistical point of view.

Armed with these preliminaries, we were set to start the task of detecting the signatures of WDM in the IGM density field in Section \ref{chap: deep learning}. This involves transforming the Lyman-$\alpha$ information into density information. For this highly non-trivial task, we resorted to neural networks. We motivated their growing use in astronomy and introduced their basic working principles. Then, we focused on how to build a robust Bayesian neural network to recover the optical depth-weighted density from the Lyman-$\alpha$ forest flux. Bayesian networks naturally allow for a quantification of the prediction uncertainty, which is especially relevant when the real data is contaminated by observational effects, such as noise. We discussed the training procedure, based on the \texttt{SHERWOOD} suite, and the hyper-parameter optimisation of the neural network. We then evaluated the accuracy of the trained neural network on the validation data, which is as high $79 \%$ ($1\sigma$), and explored the accuracy of the recovered statistics on the density field and their respective uncertainties. Lastly, we discussed some aspects of the model's interpretability, illustrated by saliency maps and pruning. All in all, in Section \ref{chap: deep learning} we find that it is viable to recover the $\Delta_\tau$ density field from Lyman-$\alpha$ skewers for different WDM models and IGM temperatures, showing that the thermal history and WDM models are not completely degenerate.

In Section \ref{sec: inference pipeline} we leverage our machine learning-recovered density fields to constrain WDM. We build an efficient statistical inference pipeline that fits the recovered $\Delta_\tau$ PDF to the PDF for each WDM model in the \texttt{SHERWOOD} suite and generates constraints on the allowed WDM particle mass. We extensively test the robustness of this pipeline using a set of CDM cosmological simulations obtained from \texttt{Nyx}, an alternative hydrodynamical code based on the Lagrangian framework. For a set of 450 \texttt{SHERWOOD} CDM simulated skewers with $SNR=30$ and a resolution of $6$ km/s per pixel, we find a lower bound on the WDM particle mass of $\sim 10$ KeV at $2\sigma$ confidence. Once successfully tested on simulated data, we apply our inference pipeline to real observational from the UltraViolet-Visual Echelle Spectrograph at the Very Large Telescope and the Gemini High-resolution Optical SpecTrograph at the Gemini South Telescope. The two independent datasets consist of 6 quasar SQUAD DR1 Lyman-$\alpha$ forests at $z=4.4$, and a single target quasar with a forest spanning the redshift range $\sim 4.1-4.9$, respectively. We obtain $2\sigma$ constraints of 3.8 and 4.4 KeV for both of those datasets. Our findings are comparable to state-of-the-art techniques, but require significantly less observed data, highlighting the efficiency of machine learning techniques in extracting information from complex data. We conclude our work by testing a statistic-independent alternative approach using Information Maximising Neural Networks to test whether our constraints can be tightened, but find no significant improvement.

Our work aligns with the ground-breaking introduction of machine learning methods in astronomy and astrophysics in recent years and highlights how such approaches can be used to efficiently extract information from the Lyman-$\alpha$ forest. Future efforts based on this approach can be targeted at jointly constraining multiple physical parameters of interest besides WDM, such as the thermal parameters of the IGM, using a finner grid of simulations. Significant improvement can also be made in designing a fully Bayesian inference method.